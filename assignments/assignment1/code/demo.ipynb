{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class RiverSwim:\n",
    "    def __init__(self, current, seed=1234):\n",
    "        self.num_states = 6\n",
    "        self.num_actions = 2  # O <=> LEFT, 1 <=> RIGHT\n",
    "\n",
    "        # Larger current makes it harder to swim up the river\n",
    "        self.currents = ['WEAK', 'MEDIUM', 'STRONG']\n",
    "        assert current in self.currents\n",
    "        self.current = self.currents.index(current) + 1\n",
    "        assert self.current in [1, 2, 3]\n",
    "\n",
    "        # Configure reward function\n",
    "        R = np.zeros((self.num_states, self.num_actions))\n",
    "        R[0, 0] = 0.005\n",
    "        R[5, 1] = 1.\n",
    "\n",
    "        # Configure transition function\n",
    "        T = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "\n",
    "        # Encode initial and rewarding state transitions\n",
    "        T[0, 0, 0] = 1.\n",
    "        T[0, 1, 0] = 0.6\n",
    "        T[0, 1, 1] = 0.4\n",
    "\n",
    "        T[5, 1, 5] = 0.6\n",
    "        T[5, 1, 4] = 0.4\n",
    "        T[5, 0, 4] = 1.\n",
    "\n",
    "        # Encode intermediate state transitions\n",
    "        for s in range(1, self.num_states - 1):\n",
    "            left, right = 0, 1\n",
    "\n",
    "            # Going left always succeeds\n",
    "            T[s, left, s - 1] = 1.\n",
    "\n",
    "            # Going right sometimes succeeds\n",
    "            T[s, right, s] = 0.6\n",
    "            T[s, right, s - 1] = 0.09 * self.current\n",
    "            T[s, right, s + 1] = 0.4 - T[s, right, s - 1]\n",
    "            assert np.isclose(np.sum(T[s, right]), 1.)\n",
    "\n",
    "        self.R = np.array(R)\n",
    "        self.T = np.array(T)\n",
    "\n",
    "        # Agent always starts at the opposite end of the river\n",
    "        self.init_state = 0\n",
    "        self.curr_state = self.init_state\n",
    "\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def get_model(self):\n",
    "        return copy.deepcopy(self.R), copy.deepcopy(self.T)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.init_state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self.R[self.curr_state, action]\n",
    "        next_state = np.random.choice(range(self.num_states), p=self.T[self.curr_state, action])\n",
    "        self.curr_state = next_state\n",
    "        return reward, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def bellman_backup(state, action, R, T, gamma, V):\n",
    "    \"\"\"\n",
    "    Perform a single Bellman backup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state: int\n",
    "    action: int\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "    gamma: float\n",
    "    V: np.array (num_states)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    backup_val: float\n",
    "    \"\"\"\n",
    "    backup_val = 0.\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    for next_state in range(len(V)):\n",
    "        backup_val += T[state, action, next_state] * (R[state, action] + gamma * V[next_state])\n",
    "    ############################\n",
    "\n",
    "    return backup_val\n",
    "\n",
    "def policy_evaluation(policy, R, T, gamma, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Compute the value function induced by a given policy for the input MDP\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.array (num_states)\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "    gamma: float\n",
    "    tol: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    value_function = np.zeros(num_states)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            v = value_function[s]\n",
    "            action = policy[s]\n",
    "            value_function[s] = bellman_backup(s, action, R, T, gamma, value_function)\n",
    "            delta = max(delta, abs(v - value_function[s]))\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    ############################\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def policy_improvement(policy, R, T, V_policy, gamma):\n",
    "    \"\"\"\n",
    "    Given the value function induced by a given policy, perform policy improvement\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.array (num_states)\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "    V_policy: np.array (num_states)\n",
    "    gamma: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    new_policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    for s in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            action_values[a] = bellman_backup(s, a, R, T, gamma, V_policy)\n",
    "        new_policy[s] = np.argmax(action_values)\n",
    "    ############################\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(R, T, gamma, tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should call the policy_evaluation() and policy_improvement() methods to\n",
    "    implement this method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V_policy: np.array (num_states)\n",
    "    policy: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    V_policy = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    is_policy_stable = False\n",
    "    while not is_policy_stable:\n",
    "        V_policy = policy_evaluation(policy, R, T, gamma) # calculate value function in current policy\n",
    "        new_policy = policy_improvement(policy, R, T, V_policy, gamma) # improve policy based on value function\n",
    "        is_policy_stable = np.all(policy == new_policy)\n",
    "        policy = new_policy\n",
    "    ############################\n",
    "    return V_policy, policy\n",
    "\n",
    "\n",
    "def value_iteration(R, T, gamma, tol=1e-3):\n",
    "    \"\"\"Runs value iteration.\n",
    "    Parameters\n",
    "    ----------\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.array (num_states)\n",
    "    policy: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    value_function = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            v = value_function[s]\n",
    "            action_values = np.zeros(num_actions)\n",
    "            for a in range(num_actions):\n",
    "                action_values[a] = bellman_backup(s, a, R, T, gamma, value_function)\n",
    "            value_function[s] = np.max(action_values)\n",
    "            policy[s] = np.argmax(action_values)\n",
    "            delta = max(delta, abs(v - value_function[s]))\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "            \n",
    "\n",
    "    ############################\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit below to run policy and value iteration on different configurations\n",
    "# You may change the parameters in the functions below\n",
    "SEED = 1234\n",
    "\n",
    "RIVER_CURRENT = 'WEAK'\n",
    "assert RIVER_CURRENT in ['WEAK', 'MEDIUM', 'STRONG']\n",
    "env = RiverSwim(RIVER_CURRENT, SEED)\n",
    "\n",
    "R, T = env.get_model()\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.348 31.116 32.356 33.774 35.288 36.881]\n",
      "['R', 'R', 'R', 'R', 'R', 'R']\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Policy Iteration\\n\" + \"-\" * 25)\n",
    "\n",
    "V_pi, policy_pi = policy_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
    "print(V_pi)\n",
    "print([['L', 'R'][a] for a in policy_pi])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.348 31.116 32.356 33.774 35.288 36.881]\n",
      "['R', 'R', 'R', 'R', 'R', 'R']\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Value Iteration\\n\" + \"-\" * 25)\n",
    "\n",
    "V_vi, policy_vi = value_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
    "print(V_vi)\n",
    "print([['L', 'R'][a] for a in policy_vi])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
